---
global:
  tolerations:
    - key: "node-role.kubernetes.io/storage"
      operator: "Exists"
      effect: "NoSchedule"

csi:
  attacherReplicaCount: 2
  provisionerReplicaCount: 2
  resizerReplicaCount: 2
  snapshotterReplicaCount: 2
  kubeletRootDir: /var/lib/kubelet
  plugin:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 128Mi
  provisioner:
    resources:
      requests:
        cpu: 55m
        memory: 68Mi
      limits:
        cpu: 55m
        memory: 68Mi
  attacher:
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 50m
        memory: 64Mi
  resizer:
    resources:
      requests:
        cpu: 45m
        memory: 60Mi
      limits:
        cpu: 45m
        memory: 60Mi
  snapshotter:
    resources:
      requests:
        cpu: 60m
        memory: 70Mi
      limits:
        cpu: 60m
        memory: 70Mi
defaultBackupStore:
  backupTarget: "nfs://10.1.30.50:/volume2/k8s/homelab/prod/longhorn/backups"
  pollInterval: 180
defaultSettings:
  allowCollectingLonghornUsageMetrics: false
  createDefaultDiskLabeledNodes: true
  defaultReplicaCount: "2"
  guaranteedEngineManagerCPU: 0.2
  guaranteedReplicaManagerCPU: 0.2
  storageOverProvisioningPercentage: "200"
  storageMinimalAvailablePercentage: "10"
  taintToleration: "node-role.kubernetes.io/storage:NoSchedule"
  defaultDataPath: /var/lib/longhorn/
  autoDeletePodWhenVolumeDetachedUnexpectedly: true
  autoSalvage: true
  replicaZoneSoftAntiAffinity: true
  replicaDiskSoftAntiAffinity: false
  concurrentReplicaRebuildPerNodeLimit: "1"
  replicaReplenishmentWaitInterval: "600"
  backupstorePollInterval: 300
  replicaSoftAntiAffinity: true
persistence:
  defaultClass: true
  defaultClassReplicaCount: 2
  reclaimPolicy: Retain
  defaultFsType: ext4
  snapshotMaxCount: 60
  defaultDataLocality: disabled # disabled: tryes to use storage on the same node as the pod

resources:
  requests:
    cpu: 50m
    memory: 64Mi
  limits:
    cpu: 100m
    memory: 128Mi

service:
  ui:
    type: LoadBalancer

longhornManager:
  priorityClass: system-cluster-critical
  resources:
    requests:
      cpu: 250m
      memory: 256Mi
    limits:
      cpu: 250m
      memory: 256Mi

longhornDriver:
  priorityClass: system-cluster-critical

preUpgradeChecker:
  jobEnabled: false

# Longhorn 1.10.1 CRD conversion configuration
# Disables webhook-based conversion to avoid Kubernetes API validation errors
crdConversion:
  enabled: false

longhornUI:
  replicas: 2

metrics:
  serviceMonitor:
    enabled: true
    additionalLabels: {}
    annotations: {}
    interval: "30s"
    scrapeTimeout: "10s"
    relabelings: []
    metricRelabelings: []
# ---
# PodSecurityContext/PodSecurity Admission Compliance
# The Longhorn Helm chart does not expose direct securityContext fields for DaemonSet/Jobs.
# To enforce PodSecurity restricted baseline, use a Helm post-render patch or Kustomize overlay.
# Example Kustomize patch (apply in overlays/longhorn):
#
# apiVersion: apps/v1
# kind: DaemonSet
# metadata:
#   name: longhorn-manager
#   namespace: longhorn-system
# spec:
#   template:
#     spec:
#       containers:
#         - name: longhorn-manager
#           securityContext:
#             runAsNonRoot: true
#             allowPrivilegeEscalation: false
#             seccompProfile:
#               type: RuntimeDefault
#             capabilities:
#               drop: ["ALL"]
#
# Repeat for any Job templates as needed.
#
# This is a best-practice deviation due to Helm chart limitations. Track for future chart updates.
